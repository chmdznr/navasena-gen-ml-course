{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Nvidia NeMo: A Powerful Toolkit for Building Conversational AI Applications\n",
        "\n",
        "Nvidia NeMo (Neural Modules) is an open-source toolkit for building state-of-the-art conversational AI applications. It provides a collection of pre-trained models and building blocks for tasks like speech recognition, natural language understanding, text-to-speech, and more.\n",
        "\n",
        "**Usage:**\n",
        "\n",
        "NeMo allows developers and researchers to quickly build and deploy advanced AI applications with minimal effort. Here's how it can be used:\n",
        "\n",
        "* **Speech Recognition:** Develop accurate and robust speech recognition systems for various languages and accents.\n",
        "* **Natural Language Understanding:** Build models for tasks like intent classification, named entity recognition, and question answering.\n",
        "* **Text-to-Speech:** Generate high-quality and natural-sounding speech from text.\n",
        "* **Dialogue Systems:** Create chatbots and virtual assistants that can engage in natural and meaningful conversations.\n",
        "* **Custom Model Training:** Leverage pre-trained models and fine-tune them for specific tasks and datasets.\n",
        "\n",
        "\n",
        "**Why Use NeMo?**\n",
        "\n",
        "* **Ease of Use:** Provides a modular and intuitive API for building and training AI models.\n",
        "* **Pre-trained Models:** Offers a collection of high-quality pre-trained models for various tasks, reducing the need for extensive training from scratch.\n",
        "* **Scalability:** Designed to leverage Nvidia's GPUs for fast and efficient training and inference.\n",
        "* **Customization:** Allows for flexibility in adapting models to specific needs and datasets.\n",
        "* **Active Development and Community Support:** Backed by Nvidia and a growing community of developers and researchers, ensuring continuous improvements and updates.\n",
        "\n",
        "**In essence, NeMo simplifies the process of developing and deploying powerful conversational AI applications. It empowers developers to focus on building innovative solutions rather than getting bogged down with complex model development and training.**"
      ],
      "metadata": {
        "id": "wpFm_aeQuzDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install base dependencies"
      ],
      "metadata": {
        "id": "M9w1tyR8q9Ts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FkcSmrHuI7N",
        "outputId": "9f6cde8e-779d-4538-edde-e1e4eab7357c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.1)\n",
            "Requirement already satisfied: nemo_toolkit[all] in /usr/local/lib/python3.10/dist-packages (1.23.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.24.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.8.2)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.18.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (75.1.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (4.66.5)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.1.0)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.16.0)\n",
            "Requirement already satisfied: black==19.10b0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (19.10b0)\n",
            "Requirement already satisfied: click==8.0.2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (8.0.2)\n",
            "Requirement already satisfied: isort<6.0.0,>5.1.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (5.13.2)\n",
            "Requirement already satisfied: parameterized in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.9.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (7.4.4)\n",
            "Requirement already satisfied: pytest-runner in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (6.0.1)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (5.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-bibtex in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.6.3)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.18.5)\n",
            "Requirement already satisfied: hydra-core<=1.3.2,>1.3 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.3.2)\n",
            "Requirement already satisfied: omegaconf<=2.3 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.3.0)\n",
            "Requirement already satisfied: pytorch-lightning<=2.0.7,>=2.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.0.7)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.5.1)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (4.44.2)\n",
            "Requirement already satisfied: webdataset<=0.1.62,>=0.1.48 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.1.62)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.0.2)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (7.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.2.2)\n",
            "Requirement already satisfied: sacremoses>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.1.1)\n",
            "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.2.0)\n",
            "Requirement already satisfied: youtokentome>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.0.6)\n",
            "Requirement already satisfied: braceexpand in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.1.7)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.8.1)\n",
            "Requirement already satisfied: g2p-en in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.1.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (7.7.1)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.5.2)\n",
            "Requirement already satisfied: kaldi-python-io in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.2.2)\n",
            "Requirement already satisfied: kaldiio in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.18.0)\n",
            "Requirement already satisfied: lhotse>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.27.0)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.10.2.post1)\n",
            "Requirement already satisfied: marshmallow in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (24.1)\n",
            "Requirement already satisfied: pyannote.core in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (5.0.0)\n",
            "Requirement already satisfied: pyannote.metrics in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.2.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.25.1)\n",
            "Requirement already satisfied: pyloudnorm in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.1.1)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.13.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.12.1)\n",
            "Requirement already satisfied: sox in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.5.0)\n",
            "Requirement already satisfied: texterrors in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.5.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.35.47)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.8.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.9.0)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.9.3)\n",
            "Requirement already satisfied: flask-restful in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.3.10)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (6.3.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (5.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.11.0)\n",
            "Requirement already satisfied: ijson in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.3.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.42.1)\n",
            "Requirement already satisfied: markdown2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.5.1)\n",
            "Requirement already satisfied: megatron-core==0.5.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.5.0)\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.8.1)\n",
            "Requirement already satisfied: opencc<1.1.7 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.1.6)\n",
            "Requirement already satisfied: pangu in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (4.0.6.1)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.13.7)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.4.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (3.2.1)\n",
            "Requirement already satisfied: tensorstore<0.1.46 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.1.45)\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.18.3)\n",
            "Requirement already satisfied: attrdict in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.0.1)\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.7.3)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.53.0)\n",
            "Requirement already satisfied: pypinyin-dict in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.8.0)\n",
            "Requirement already satisfied: progress>=1.5 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.6)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.9.0)\n",
            "Requirement already satisfied: textdistance>=4.1.5 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (4.6.3)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.4.0)\n",
            "Requirement already satisfied: clip in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.2.0)\n",
            "Requirement already satisfied: diffusers>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.30.3)\n",
            "Requirement already satisfied: einops-exts in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.0.4)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.35.1)\n",
            "Requirement already satisfied: nerfacc>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.5.3)\n",
            "Requirement already satisfied: open-clip-torch in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (2.28.0)\n",
            "Requirement already satisfied: PyMCubes in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.1.6)\n",
            "Requirement already satisfied: taming-transformers in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.0.1)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.2.4)\n",
            "Requirement already satisfied: torchsde in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (0.2.6)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (4.5.1)\n",
            "Requirement already satisfied: nemo-text-processing in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[all]) (1.1.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from black==19.10b0->nemo_toolkit[all]) (24.2.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from black==19.10b0->nemo_toolkit[all]) (1.4.4)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.10/dist-packages (from black==19.10b0->nemo_toolkit[all]) (0.10.2)\n",
            "Requirement already satisfied: typed-ast>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from black==19.10b0->nemo_toolkit[all]) (1.5.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from black==19.10b0->nemo_toolkit[all]) (2024.9.11)\n",
            "Requirement already satisfied: pathspec<1,>=0.6 in /usr/local/lib/python3.10/dist-packages (from black==19.10b0->nemo_toolkit[all]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.19.3->nemo_toolkit[all]) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->nemo_toolkit[all]) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->nemo_toolkit[all]) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->nemo_toolkit[all]) (4.12.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core<=1.3.2,>1.3->nemo_toolkit[all]) (4.9.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from lhotse>=1.20.0->nemo_toolkit[all]) (3.0.1)\n",
            "Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from lhotse>=1.20.0->nemo_toolkit[all]) (1.0.0)\n",
            "Requirement already satisfied: intervaltree>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from lhotse>=1.20.0->nemo_toolkit[all]) (3.1.0)\n",
            "Requirement already satisfied: lilcom>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lhotse>=1.20.0->nemo_toolkit[all]) (1.8.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo_toolkit[all]) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo_toolkit[all]) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo_toolkit[all]) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo_toolkit[all]) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo_toolkit[all]) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo_toolkit[all]) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[all]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[all]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[all]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[all]) (3.2.0)\n",
            "Requirement already satisfied: rich>=12 in /usr/local/lib/python3.10/dist-packages (from nerfacc>=0.5.3->nemo_toolkit[all]) (13.9.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->nemo_toolkit[all]) (0.43.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.7.0->nemo_toolkit[all]) (3.20.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->nemo_toolkit[all]) (1.16.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=2.0.7,>=2.0->nemo_toolkit[all]) (0.11.8)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo_toolkit[all]) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->nemo_toolkit[all]) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[all]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[all]) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->nemo_toolkit[all]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->nemo_toolkit[all]) (0.19.1)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.47 in /usr/local/lib/python3.10/dist-packages (from boto3->nemo_toolkit[all]) (1.35.47)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->nemo_toolkit[all]) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->nemo_toolkit[all]) (0.10.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->nemo_toolkit[all]) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->nemo_toolkit[all]) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->nemo_toolkit[all]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->nemo_toolkit[all]) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->nemo_toolkit[all]) (3.10.10)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext->nemo_toolkit[all]) (2.13.6)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[all]) (9.0.1)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[all]) (2.2.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[all]) (2024.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->nemo_toolkit[all]) (0.2.13)\n",
            "Requirement already satisfied: distance>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from g2p-en->nemo_toolkit[all]) (0.1.3)\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect->nemo_toolkit[all]) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect->nemo_toolkit[all]) (4.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->nemo_toolkit[all]) (4.12.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[all]) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[all]) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[all]) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[all]) (3.6.9)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[all]) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[all]) (3.0.13)\n",
            "Requirement already satisfied: kornia-rs>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from kornia->nemo_toolkit[all]) (0.1.5)\n",
            "Requirement already satisfied: cdifflib in /usr/local/lib/python3.10/dist-packages (from nemo-text-processing->nemo_toolkit[all]) (1.2.6)\n",
            "Requirement already satisfied: pynini==2.1.6.post1 in /usr/local/lib/python3.10/dist-packages (from nemo-text-processing->nemo_toolkit[all]) (2.1.6.post1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch->nemo_toolkit[all]) (0.20.0+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch->nemo_toolkit[all]) (1.0.11)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo_toolkit[all]) (2024.2)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core->nemo_toolkit[all]) (2.4.0)\n",
            "Requirement already satisfied: pyannote.database>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo_toolkit[all]) (5.1.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo_toolkit[all]) (0.6.2)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from pyloudnorm->nemo_toolkit[all]) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->nemo_toolkit[all]) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->nemo_toolkit[all]) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->nemo_toolkit[all]) (2.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->nemo_toolkit[all]) (1.4.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml->nemo_toolkit[all]) (0.2.12)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->nemo_toolkit[all]) (2.10.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->nemo_toolkit[all]) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->nemo_toolkit[all]) (4.9.4)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (0.17.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx->nemo_toolkit[all]) (1.4.1)\n",
            "Requirement already satisfied: pybtex>=0.24 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex->nemo_toolkit[all]) (0.24.0)\n",
            "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-bibtex->nemo_toolkit[all]) (1.0.3)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[all]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[all]) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[all]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[all]) (3.0.4)\n",
            "Requirement already satisfied: plac in /usr/local/lib/python3.10/dist-packages (from texterrors->nemo_toolkit[all]) (1.4.3)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from texterrors->nemo_toolkit[all]) (0.7.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from texterrors->nemo_toolkit[all]) (2.5.0)\n",
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (from texterrors->nemo_toolkit[all]) (0.22.0)\n",
            "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from torchsde->nemo_toolkit[all]) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (4.3.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[all]) (1.3.3)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr->nemo_toolkit[all]) (0.3.3)\n",
            "Requirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr->nemo_toolkit[all]) (0.13.1)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr->nemo_toolkit[all]) (0.19)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.47->boto3->nemo_toolkit[all]) (2.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit[all]) (2.22)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->lhotse>=1.20.0->nemo_toolkit[all]) (0.12.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->nemo_toolkit[all]) (2.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo_toolkit[all]) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[all]) (4.0.11)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[all]) (6.3.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.19.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (3.0.48)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->nemo_toolkit[all]) (3.0.2)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[all]) (0.12.5)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from pybtex>=0.24->sphinxcontrib-bibtex->nemo_toolkit[all]) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.19.3->nemo_toolkit[all]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.19.3->nemo_toolkit[all]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.19.3->nemo_toolkit[all]) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12->nerfacc>=0.5.3->nemo_toolkit[all]) (3.0.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (6.5.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->nemo_toolkit[all]) (2.6)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers>=0.19.3->nemo_toolkit[all]) (3.20.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->nemo_toolkit[all]) (1.7.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[all]) (5.0.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12->nerfacc>=0.5.3->nemo_toolkit[all]) (0.1.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->nemo_toolkit[all]) (0.7.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[all]) (1.5.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->nemo_toolkit[all]) (0.2.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.2.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (21.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.24.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (0.5.1)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[all]) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# Install NeMo and required dependencies\n",
        "!apt-get update && apt-get install -y libsndfile1 ffmpeg\n",
        "!pip install Cython packaging\n",
        "!pip install nemo_toolkit['all']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downgrading some libraries"
      ],
      "metadata": {
        "id": "gwP0DfGuq3Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub==0.23.2\n",
        "!pip install transformers==4.40.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET6FpUVkv81s",
        "outputId": "7fede27f-8284-464d-b9dc-ae792b56cf4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface-hub==0.23.2\n",
            "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (2024.8.30)\n",
            "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed huggingface-hub-0.23.2\n",
            "Collecting transformers==4.40.0\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.8.30)\n",
            "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main imports"
      ],
      "metadata": {
        "id": "dcP_05V8ExOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from nemo.collections.nlp.models import TokenClassificationModel, TextClassificationModel\n",
        "from nemo.collections.nlp.models import QAModel, PunctuationCapitalizationModel\n",
        "from nemo.collections.nlp.models.machine_translation import MTEncDecModel\n",
        "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
        "from nemo.collections.asr.models import EncDecCTCModel\n",
        "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
      ],
      "metadata": {
        "id": "I2vJScRwuUUm",
        "outputId": "eeec49f8-3ebc-49f3-f09b-e2925ba90b95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:17:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-10-24 13:17:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-10-24 13:17:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-10-24 13:17:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-10-24 13:17:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/tts/modules/common.py:206: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "      @amp.autocast(False)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "os.environ['NEMO_LOGGING_LEVEL'] = 'ERROR'  # Reduces verbosity"
      ],
      "metadata": {
        "id": "2RxiAzAU3OJk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Demo Class"
      ],
      "metadata": {
        "id": "UzHlR2Ll8v4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NeMoDemo:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize NeMo demo with proper environment setup\"\"\"\n",
        "        # Set environment variables\n",
        "        os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "        os.environ['NEMO_LOGGING_LEVEL'] = 'ERROR'  # Reduces verbosity\n",
        "\n",
        "        # Setup device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Print GPU info if available\n",
        "        if self.device == 'cuda':\n",
        "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "            print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "            print(f\"Available GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB used\")\n",
        "\n",
        "    def setup_and_verify(self):\n",
        "        \"\"\"Verify NeMo setup and GPU availability\"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== System Information ===\")\n",
        "            print(f\"PyTorch version: {torch.__version__}\")\n",
        "            print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "            if torch.cuda.is_available():\n",
        "                print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "                print(f\"CUDA version: {torch.version.cuda}\")\n",
        "                print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
        "                print(f\"Memory cached: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
        "\n",
        "            # Test basic model loading\n",
        "            print(\"\\n=== Testing Model Loading ===\")\n",
        "            try:\n",
        "                test_model = TokenClassificationModel.from_pretrained(\"ner_en_bert\")\n",
        "                print(\"✓ Successfully loaded test model (NER)\")\n",
        "                del test_model\n",
        "                torch.cuda.empty_cache()\n",
        "            except Exception as e:\n",
        "                print(f\"× Error loading test model: {str(e)}\")\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error in setup and verification: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    # Optional: Helper function to inspect model methods\n",
        "    def inspect_qa_model(self, model):\n",
        "        \"\"\"Inspect QA model methods and properties\"\"\"\n",
        "        print(\"\\n=== QA Model Inspection ===\")\n",
        "\n",
        "        # Get all public methods\n",
        "        methods = [method for method in dir(model) if not method.startswith('_')]\n",
        "\n",
        "        print(\"\\nPublic Methods:\")\n",
        "        for method in methods:\n",
        "            try:\n",
        "                attr = getattr(model, method)\n",
        "                if callable(attr):\n",
        "                    import inspect\n",
        "                    sig = inspect.signature(attr)\n",
        "                    print(f\"{method}{sig}\")\n",
        "            except Exception as e:\n",
        "                print(f\"{method}: Could not get signature - {str(e)}\")\n",
        "\n",
        "        # Get model configuration\n",
        "        if hasattr(model, 'cfg'):\n",
        "            print(\"\\nModel Configuration:\")\n",
        "            print(model.cfg)\n",
        "\n",
        "        # Get model parameters\n",
        "        print(\"\\nModel Parameters:\")\n",
        "        for name, param in model.named_parameters():\n",
        "            print(f\"{name}: {param.shape}\")\n",
        "\n",
        "    def demonstrate_question_answering(self):\n",
        "      \"\"\"Question Answering demo with NeMo QAModel\"\"\"\n",
        "\n",
        "      print(\"\\n=== Question Answering Demo ===\")\n",
        "\n",
        "      # Load NeMo QA model\n",
        "      model = QAModel.from_pretrained(\"qa_squadv1.1_bertbase\")\n",
        "      model.eval()\n",
        "\n",
        "      # Load the HuggingFace tokenizer separately\n",
        "      tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "      # Example context and questions\n",
        "      context = \"\"\"\n",
        "      NVIDIA Corporation is a technology company founded in 1993.\n",
        "      The company designs graphics processing units (GPUs) for gaming\n",
        "      and professional markets, as well as system on chip units (SoCs)\n",
        "      for the mobile computing and automotive market. The company's\n",
        "      headquarters are located in Santa Clara, California.\n",
        "      \"\"\"\n",
        "\n",
        "      questions = [\n",
        "          \"When was NVIDIA founded?\",\n",
        "          \"What does NVIDIA design?\",\n",
        "          \"Where is NVIDIA headquartered?\",\n",
        "          \"Who is the founder of NVIDIA?\"\n",
        "      ]\n",
        "\n",
        "      # Process each question\n",
        "      for question in questions:\n",
        "\n",
        "          # Tokenize the question and context using HuggingFace's tokenizer\n",
        "          inputs = tokenizer(\n",
        "              question, context, add_special_tokens=True,\n",
        "              max_length=512, truncation=True, return_tensors='pt'\n",
        "          )\n",
        "\n",
        "          # Move inputs to model's device\n",
        "          input_ids = inputs['input_ids'].to(model.device)\n",
        "          attention_mask = inputs['attention_mask'].to(model.device)\n",
        "          token_type_ids = inputs['token_type_ids'].to(model.device)\n",
        "\n",
        "          # Forward pass through the model\n",
        "          with torch.no_grad():\n",
        "              # Now include token_type_ids in the model input\n",
        "              outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "              # The output shape is [batch_size, sequence_length, 2]\n",
        "              # Split the tensor into start and end logits\n",
        "              start_logits, end_logits = outputs[..., 0], outputs[..., 1]\n",
        "\n",
        "          # Get the most probable start and end positions\n",
        "          start_idx = torch.argmax(start_logits)\n",
        "          end_idx = torch.argmax(end_logits)\n",
        "\n",
        "          # Convert the token IDs back to tokens\n",
        "          input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "          answer_tokens = input_tokens[start_idx:end_idx + 1]\n",
        "\n",
        "          # Join the tokens to form the answer\n",
        "          answer = tokenizer.convert_tokens_to_string(answer_tokens).strip()\n",
        "\n",
        "          print(f\"\\nQuestion: {question}\")\n",
        "          print(f\"Answer: {answer}\")\n",
        "\n",
        "\n",
        "    def demonstrate_ner(self):\n",
        "        \"\"\"Named Entity Recognition demo\"\"\"\n",
        "        print(\"\\n=== Named Entity Recognition Demo ===\")\n",
        "        try:\n",
        "            model = TokenClassificationModel.from_pretrained(\"ner_en_bert\")\n",
        "            texts = [\n",
        "                \"NVIDIA released its new GPU in New York last September.\",\n",
        "                \"Jensen Huang discussed AI developments at GTC in San Francisco.\"\n",
        "            ]\n",
        "            results = model.add_predictions(texts)\n",
        "            for text, entities in zip(texts, results):\n",
        "                print(f\"\\nText: {text}\")\n",
        "                print(\"Entities:\", entities)\n",
        "        except Exception as e:\n",
        "            print(f\"NER Error: {str(e)}\")\n",
        "\n",
        "    def demonstrate_text_classification(self):\n",
        "        \"\"\"Text Classification demo with improved error handling\"\"\"\n",
        "        print(\"\\n=== Text Classification Demo ===\")\n",
        "        try:\n",
        "            # Check available models first\n",
        "            available_models = TextClassificationModel.list_available_models()\n",
        "            if not available_models:\n",
        "                print(\"No Text Classification models available\")\n",
        "                return\n",
        "\n",
        "            model = TextClassificationModel.from_pretrained(available_models[0])\n",
        "            model.eval()\n",
        "\n",
        "            texts = [\n",
        "                \"This GPU performs incredibly well for deep learning tasks.\",\n",
        "                \"The customer service was very disappointing.\",\n",
        "                \"The product arrived on time but was damaged.\"\n",
        "            ]\n",
        "\n",
        "            print(\"\\nAnalyzing texts...\")\n",
        "            for text in texts:\n",
        "                try:\n",
        "                    prediction = model.classifying([text])[0]\n",
        "                    print(f\"\\nText: {text}\")\n",
        "                    print(f\"Classification: {prediction}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error classifying text: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Classification Error: {str(e)}\")\n",
        "\n",
        "    def demonstrate_punctuation_capitalization(self):\n",
        "        \"\"\"Punctuation and Capitalization demo\"\"\"\n",
        "        print(\"\\n=== Punctuation and Capitalization Demo ===\")\n",
        "        try:\n",
        "            model = PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_bert\")\n",
        "            model.eval()\n",
        "\n",
        "            texts = [\n",
        "                \"nvidia is a technology company based in santa clara california that develops gpus\",\n",
        "                \"ai and deep learning are transforming various industries globally\"\n",
        "            ]\n",
        "\n",
        "            print(\"\\nProcessing texts...\")\n",
        "            for text in texts:\n",
        "                try:\n",
        "                    processed_text = model.add_punctuation_capitalization([text])[0]\n",
        "                    print(f\"\\nOriginal: {text}\")\n",
        "                    print(f\"Processed: {processed_text}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing text: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Punctuation Error: {str(e)}\")\n",
        "\n",
        "    def demonstrate_machine_translation(self):\n",
        "        \"\"\"Machine Translation demo\"\"\"\n",
        "        print(\"\\n=== Machine Translation Demo ===\")\n",
        "        try:\n",
        "            available_models = MTEncDecModel.list_available_models()\n",
        "            if not available_models:\n",
        "                print(\"No Translation models available\")\n",
        "                return\n",
        "\n",
        "            model = MTEncDecModel.from_pretrained(\"nmt_en_zh_transformer24x6\")\n",
        "            model.eval()\n",
        "\n",
        "            texts = [\n",
        "                \"NVIDIA GPUs are excellent for deep learning.\",\n",
        "                \"Artificial intelligence is transforming the technology landscape.\"\n",
        "            ]\n",
        "\n",
        "            print(\"\\nTranslating texts...\")\n",
        "            for text in texts:\n",
        "                try:\n",
        "                    translation = model.translate([text])[0]\n",
        "                    print(f\"\\nEnglish: {text}\")\n",
        "                    print(f\"Translated: {translation}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error translating text: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Translation Error: {str(e)}\")\n",
        "\n",
        "    def demonstrate_text_to_speech(self):\n",
        "        \"\"\"Text-to-Speech demo\"\"\"\n",
        "        print(\"\\n=== Text-to-Speech Demo ===\")\n",
        "        try:\n",
        "            # Load FastPitch and HifiGan models\n",
        "            spec_generator = FastPitchModel.from_pretrained(\"tts_en_fastpitch\")\n",
        "            vocoder = HifiGanModel.from_pretrained(\"tts_en_hifigan\")\n",
        "\n",
        "            spec_generator.eval()\n",
        "            vocoder.eval()\n",
        "\n",
        "            # text = \"Welcome to the NVIDIA NeMo demonstration!\"\n",
        "            text = \"How are you, guys?\"\n",
        "            print(f\"Converting text to speech: '{text}'\")\n",
        "\n",
        "            try:\n",
        "                # Generate spectrogram\n",
        "                with torch.no_grad():\n",
        "                    parsed = spec_generator.parse(text)\n",
        "                    spectrogram = spec_generator.generate_spectrogram(tokens=parsed)\n",
        "                    audio = vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
        "\n",
        "                print(\"✓ Audio generation successful!\")\n",
        "                print(\"Note: Audio playback is not available in this environment\")\n",
        "                # For environments that support audio playback (e.g., Colab):\n",
        "                from IPython.display import Audio\n",
        "                display(Audio(audio.cpu().numpy()[0], rate=22050))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating audio: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"TTS Error: {str(e)}\")\n",
        "\n",
        "    def list_available_models(self):\n",
        "        \"\"\"List all available models for each task\"\"\"\n",
        "        print(\"\\n=== Available Models ===\")\n",
        "\n",
        "        model_types = {\n",
        "            \"NER Models\": TokenClassificationModel,\n",
        "            \"QA Models\": QAModel,\n",
        "            \"Text Classification Models\": TextClassificationModel,\n",
        "            \"Punctuation and Capitalization Models\": PunctuationCapitalizationModel,\n",
        "            \"Translation Models\": MTEncDecModel,\n",
        "            \"TTS Models\": FastPitchModel\n",
        "        }\n",
        "\n",
        "        for model_name, model_class in model_types.items():\n",
        "            print(f\"\\n{model_name}:\")\n",
        "            try:\n",
        "                models = model_class.list_available_models()\n",
        "                if isinstance(models, list):\n",
        "                    for model in models:\n",
        "                        print(f\"- {model}\")\n",
        "                else:\n",
        "                    print(f\"- {models}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error listing {model_name}: {str(e)}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        print(\"\\n=== Cleanup ===\")\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"✓ GPU memory cleared\")"
      ],
      "metadata": {
        "id": "_sGH0S6WvFcX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup NeMo"
      ],
      "metadata": {
        "id": "CSYaPhGWDTjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize demo\n",
        "demo = NeMoDemo()\n",
        "\n",
        "# Setup and verify\n",
        "if not demo.setup_and_verify():\n",
        "  print(\"Setup failed. Exiting.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EccN4SXBSU_",
        "outputId": "2ca9b520-5eb6-41e5-f308-8edc38b2fdb6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Total GPU memory: 15.84 GB\n",
            "Available GPU memory: 1.05 GB used\n",
            "\n",
            "=== System Information ===\n",
            "PyTorch version: 2.5.0+cu121\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "CUDA version: 12.1\n",
            "Memory allocated: 1.05 GB\n",
            "Memory cached: 2.88 GB\n",
            "\n",
            "=== Testing Model Loading ===\n",
            "[NeMo I 2024-10-24 13:48:43 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.23.0/ner_en_bert/8186f86c83b11d70b43b9ead695e7eda/ner_en_bert.nemo.\n",
            "[NeMo I 2024-10-24 13:48:43 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.23.0/ner_en_bert/8186f86c83b11d70b43b9ead695e7eda/ner_en_bert.nemo\n",
            "[NeMo I 2024-10-24 13:48:43 common:924] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2024-10-24 13:48:50 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpaj7a6niz/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:48:50 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
            "[NeMo W 2024-10-24 13:48:50 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    text_file: text_train.txt\n",
            "    labels_file: labels_train.txt\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "    \n",
            "[NeMo W 2024-10-24 13:48:50 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "    \n",
            "[NeMo W 2024-10-24 13:48:50 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "    \n",
            "[NeMo W 2024-10-24 13:48:50 nlp_overrides:802] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
            "    Megatron-based models require Apex to function correctly.\n",
            "[NeMo W 2024-10-24 13:48:50 lm_utils:91] bert-base-uncased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n",
            "[NeMo W 2024-10-24 13:48:52 modelPT:258] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:48:53 save_restore_connector:249] Model TokenClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/ner_en_bert/8186f86c83b11d70b43b9ead695e7eda/ner_en_bert.nemo.\n",
            "✓ Successfully loaded test model (NER)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check avilable models"
      ],
      "metadata": {
        "id": "RUuxEMafDXAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# List available models\n",
        "demo.list_available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aPqZf90uNdt",
        "outputId": "7cdc46b8-f2f0-442c-d8a1-4100ec453382"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Available Models ===\n",
            "\n",
            "NER Models:\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=ner_en_bert,\n",
            "\tdescription=The model was trained on GMB (Groningen Meaning Bank) corpus for entity recognition and achieves 74.61 F1 Macro score.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/ner_en_bert/versions/1.10/files/ner_en_bert.nemo\n",
            ")\n",
            "\n",
            "QA Models:\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=qa_squadv1.1_bertbase,\n",
            "\tdescription=Question answering model finetuned from NeMo BERT Base Uncased on SQuAD v1.1 dataset which obtains an exact match (EM) score of 82.78% and an F1 score of 89.97%.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_bertbase/versions/1.0.0rc1/files/qa_squadv1.1_bertbase.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=qa_squadv2.0_bertbase,\n",
            "\tdescription=Question answering model finetuned from NeMo BERT Base Uncased on SQuAD v2.0 dataset which obtains an exact match (EM) score of 75.04% and an F1 score of 78.08%.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv2_0_bertbase/versions/1.0.0rc1/files/qa_squadv2.0_bertbase.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=qa_squadv1_1_bertlarge,\n",
            "\tdescription=Question answering model finetuned from NeMo BERT Large Uncased on SQuAD v1.1 dataset which obtains an exact match (EM) score of 85.44% and an F1 score of 92.06%.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_bertlarge/versions/1.0.0rc1/files/qa_squadv1.1_bertlarge.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=qa_squadv2.0_bertlarge,\n",
            "\tdescription=Question answering model finetuned from NeMo BERT Large Uncased on SQuAD v2.0 dataset which obtains an exact match (EM) score of 80.22% and an F1 score of 83.05%.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv2_0_bertlarge/versions/1.0.0rc1/files/qa_squadv2.0_bertlarge.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=qa_squadv1_1_megatron_cased,\n",
            "\tdescription=Question answering model finetuned from Megatron Cased on SQuAD v1.1 dataset which obtains an exact match (EM) score of 88.18% and an F1 score of 94.07%.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_megatron_cased/versions/1.0.0rc1/files/qa_squadv1.1_megatron_cased.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=qa_squadv2.0_megatron_cased,\n",
            "\tdescription=Question answering model finetuned from Megatron Cased on SQuAD v2.0 dataset which obtains an exact match (EM) score of 84.73% and an F1 score of 87.89%.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv2_0_megatron_cased/versions/1.0.0rc1/files/qa_squadv2.0_megatron_cased.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=qa_squadv1.1_megatron_uncased,\n",
            "\tdescription=Question answering model finetuned from Megatron Unased on SQuAD v1.1 dataset which obtains an exact match (EM) score of 87.61% and an F1 score of 94.00%.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_megatron_uncased/versions/1.0.0rc1/files/qa_squadv1.1_megatron_uncased.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=qa_squadv2.0_megatron_uncased,\n",
            "\tdescription=Question answering model finetuned from Megatron Uncased on SQuAD v2.0 dataset which obtains an exact match (EM) score of 84.48% and an F1 score of 87.65%.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv2_0_megatron_uncased/versions/1.0.0rc1/files/qa_squadv2.0_megatron_uncased.nemo\n",
            ")\n",
            "\n",
            "Text Classification Models:\n",
            "- None\n",
            "\n",
            "Punctuation and Capitalization Models:\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=punctuation_en_bert,\n",
            "\tdescription=The model was trained with NeMo BERT base uncased checkpoint on a subset of data from the following sources: Tatoeba sentences, books from Project Gutenberg, Fisher transcripts.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/punctuation_en_bert/versions/1.0.0rc1/files/punctuation_en_bert.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=punctuation_en_distilbert,\n",
            "\tdescription=The model was trained with DistilBERT base uncased checkpoint from HuggingFace on a subset of data from the following sources: Tatoeba sentences, books from Project Gutenberg, Fisher transcripts.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/punctuation_en_distilbert/versions/1.0.0rc1/files/punctuation_en_distilbert.nemo\n",
            ")\n",
            "\n",
            "Translation Models:\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_de_transformer12x2,\n",
            "\tdescription=En->De translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_de_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_de_transformer12x2/versions/1.0.0rc1/files/nmt_en_de_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_de_en_transformer12x2,\n",
            "\tdescription=De->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_de_en_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_de_en_transformer12x2/versions/1.0.0rc1/files/nmt_de_en_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_es_transformer12x2,\n",
            "\tdescription=En->Es translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_es_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_es_transformer12x2/versions/1.0.0rc1/files/nmt_en_es_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_es_en_transformer12x2,\n",
            "\tdescription=Es->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_es_en_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_es_en_transformer12x2/versions/1.0.0rc1/files/nmt_es_en_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_fr_transformer12x2,\n",
            "\tdescription=En->Fr translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_fr_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_fr_transformer12x2/versions/1.0.0rc1/files/nmt_en_fr_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_fr_en_transformer12x2,\n",
            "\tdescription=Fr->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_fr_en_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_fr_en_transformer12x2/versions/1.0.0rc1/files/nmt_fr_en_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_ru_transformer6x6,\n",
            "\tdescription=En->Ru translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_ru_transformer6x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_ru_transformer6x6/versions/1.0.0rc1/files/nmt_en_ru_transformer6x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_ru_en_transformer6x6,\n",
            "\tdescription=Ru->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_ru_en_transformer6x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_ru_en_transformer6x6/versions/1.0.0rc1/files/nmt_ru_en_transformer6x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_zh_en_transformer6x6,\n",
            "\tdescription=Zh->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_zh_en_transformer6x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_zh_en_transformer6x6/versions/1.0.0rc1/files/nmt_zh_en_transformer6x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_zh_transformer6x6,\n",
            "\tdescription=En->Zh translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_zh_transformer6x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_zh_transformer6x6/versions/1.0.0rc1/files/nmt_en_zh_transformer6x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_hi_en_transformer12x2,\n",
            "\tdescription=Hi->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_hi_en_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_hi_en_transformer12x2/versions/v1.0.0/files/nmt_hi_en_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_hi_transformer12x2,\n",
            "\tdescription=En->Hi translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_hi_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_hi_transformer12x2/versions/v1.0.0/files/nmt_en_hi_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=mnmt_deesfr_en_transformer12x2,\n",
            "\tdescription=De/Es/Fr->En multilingual many-one translation model. The model has 12 encoder and 2 decoder layers with hidden dim 1,024. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:mnmt_deesfr_en_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/mnmt_deesfr_en_transformer12x2/versions/1.2.0/files/mnmt_deesfr_en_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=mnmt_deesfr_en_transformer24x6,\n",
            "\tdescription=De/Es/Fr->En multilingual many-one translation model. The model has 24 encoder and 6 decoder layers with hidden dim 1,024. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:mnmt_deesfr_en_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/mnmt_deesfr_en_transformer24x6/versions/1.2.0/files/mnmt_deesfr_en_transformer24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=mnmt_deesfr_en_transformer6x6,\n",
            "\tdescription=De/Es/Fr->En multilingual many-one translation model. The model has 6 encoder and 6 decoder layers with hidden dim 1,024. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:mnmt_deesfr_en_transformer6x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/mnmt_deesfr_en_transformer6x6/versions/1.2.0/files/mnmt_deesfr_en_transformer6x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=mnmt_en_deesfr_transformer12x2,\n",
            "\tdescription=En->De/Es/Fr multilingual one-many translation model. The model has 12 encoder and 2 decoder layers with hidden dim 1,024. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:mnmt_en_deesfr_transformer12x2,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/mnmt_en_deesfr_transformer12x2/versions/1.2.0/files/mnmt_en_deesfr_transformer12x2.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=mnmt_en_deesfr_transformer24x6,\n",
            "\tdescription=En->De/Es/Fr multilingual one-many translation model. The model has 24 encoder and 6 decoder layers with hidden dim 1,024. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:mnmt_en_deesfr_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/mnmt_en_deesfr_transformer24x6/versions/1.2.0/files/mnmt_en_deesfr_transformer24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=mnmt_en_deesfr_transformer6x6,\n",
            "\tdescription=En->De/Es/Fr multilingual one-many translation model. The model has 6 encoder and 6 decoder layers with hidden dim 1,024. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:mnmt_en_deesfr_transformer6x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/mnmt_en_deesfr_transformer6x6/versions/1.2.0/files/mnmt_en_deesfr_transformer6x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=mnmt_en_deesfr_transformerbase,\n",
            "\tdescription=En->De/Es/Fr multilingual one-many translation model. The model has 6 encoder and 6 decoder layers with hidden dim 512. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:mnmt_en_deesfr_transformerbase,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/mnmt_en_deesfr_transformerbase/versions/1.2.0/files/mnmt_en_deesfr_transformerbase.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_de_transformer24x6,\n",
            "\tdescription=En->De translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_de_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_de_transformer24x6/versions/1.5/files/en_de_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_de_en_transformer24x6,\n",
            "\tdescription=De->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_de_en_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_de_en_transformer24x6/versions/1.5/files/de_en_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_es_transformer24x6,\n",
            "\tdescription=En->Es translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_es_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_es_transformer24x6/versions/1.5/files/en_es_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_es_en_transformer24x6,\n",
            "\tdescription=Es->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_es_en_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_es_en_transformer24x6/versions/1.5/files/es_en_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_fr_transformer24x6,\n",
            "\tdescription=En->Fr translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_fr_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_fr_transformer24x6/versions/1.5/files/en_fr_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_fr_en_transformer24x6,\n",
            "\tdescription=Fr->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_fr_en_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_fr_en_transformer24x6/versions/1.5/files/fr_en_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_ru_transformer24x6,\n",
            "\tdescription=En->Ru translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_ru_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_ru_transformer24x6/versions/1.5/files/en_ru_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_ru_en_transformer24x6,\n",
            "\tdescription=Ru->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_ru_en_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_ru_en_transformer24x6/versions/1.5/files/ru_en_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_en_zh_transformer24x6,\n",
            "\tdescription=En->Zh translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_en_zh_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_zh_transformer24x6/versions/1.5/files/en_zh_24x6.nemo\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=nmt_zh_en_transformer24x6,\n",
            "\tdescription=Zh->En translation model. See details here: https://ngc.nvidia.com/catalog/models/nvidia:nemo:nmt_zh_en_transformer24x6,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_zh_en_transformer24x6/versions/1.5/files/zh_en_24x6.nemo\n",
            ")\n",
            "\n",
            "TTS Models:\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_en_fastpitch,\n",
            "\tdescription=This model is trained on LJSpeech sampled at 22050Hz with and can be used to generate female English voices with an American accent. It is ARPABET-based.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_en_fastpitch/versions/1.8.1/files/tts_en_fastpitch_align.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_en_fastpitch_ipa,\n",
            "\tdescription=This model is trained on LJSpeech sampled at 22050Hz with and can be used to generate female English voices with an American accent. It is IPA-based.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_en_fastpitch/versions/IPA_1.13.0/files/tts_en_fastpitch_align_ipa.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_en_fastpitch_multispeaker,\n",
            "\tdescription=This model is trained on HiFITTS sampled at 44100Hz with and can be used to generate male and female English voices with an American accent.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_en_multispeaker_fastpitchhifigan/versions/1.10.0/files/tts_en_fastpitch_multispeaker.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_de_fastpitch_singleSpeaker_thorstenNeutral_2102,\n",
            "\tdescription=This model is trained on a single male speaker data in Thorsten Müller’s German Neutral 21.02 Dataset sampled at 22050Hz and can be used to generate male German voices.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_de_fastpitchhifigan/versions/1.15.0/files/tts_de_fastpitch_thorstens2102.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_de_fastpitch_singleSpeaker_thorstenNeutral_2210,\n",
            "\tdescription=This model is trained on a single male speaker data in Thorsten Müller’s German Neutral 22.10 Dataset sampled at 22050Hz and can be used to generate male German voices.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_de_fastpitchhifigan/versions/1.15.0/files/tts_de_fastpitch_thorstens2210.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_de_fastpitch_multispeaker_5,\n",
            "\tdescription=This model is trained on 5 speakers in HUI-Audio-Corpus-German clean subset sampled at 44100Hz with and can be used to generate male and female German voices.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_de_fastpitch_multispeaker_5/versions/1.11.0/files/tts_de_fastpitch_multispeaker_5.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_es_fastpitch_multispeaker,\n",
            "\tdescription=This model is trained on 174 speakers in 6 crowdsourced Latin American Spanish OpenSLR datasets sampled at 44100Hz and can be used to generate male and female Spanish voices with Latin American accents.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_es_multispeaker_fastpitchhifigan/versions/1.15.0/files/tts_es_fastpitch_multispeaker.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_zh_fastpitch_sfspeech,\n",
            "\tdescription=This model is trained on a single female speaker in SFSpeech Bilingual Chinese/English dataset sampled at 22050Hz and can be used to generate female Mandarin Chinese voices. It is improved using richer dict and jieba word segmenter for polyphone disambiguation.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_zh_fastpitch_hifigan_sfspeech/versions/1.15.0/files/tts_zh_fastpitch_sfspeech.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n",
            "- PretrainedModelInfo(\n",
            "\tpretrained_model_name=tts_en_fastpitch_for_asr_finetuning,\n",
            "\tdescription=This model is trained on LibriSpeech, train-960 subset. STFT parameters follow those commonly used in ASR: 25 ms window, 10 ms hop. This model is supposed to be used with its companion SpetrogramEnhancer for  ASR fine-tuning. Usage for regular TTS tasks is not advised.,\n",
            "\tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_en_fastpitch_spectrogram_enhancer_for_asr_finetuning/versions/1.20.0/files/tts_en_fastpitch_for_asr_finetuning.nemo,\n",
            "\tclass_=<class 'nemo.collections.tts.models.fastpitch.FastPitchModel'>\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER"
      ],
      "metadata": {
        "id": "1lU_B52gDaxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run demonstrations\n",
        "demo.demonstrate_ner()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGbhCoZy1SpL",
        "outputId": "b5a05afe-b037-439d-aebb-ed3c009b3511"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Named Entity Recognition Demo ===\n",
            "[NeMo I 2024-10-24 13:27:31 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.23.0/ner_en_bert/8186f86c83b11d70b43b9ead695e7eda/ner_en_bert.nemo.\n",
            "[NeMo I 2024-10-24 13:27:31 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.23.0/ner_en_bert/8186f86c83b11d70b43b9ead695e7eda/ner_en_bert.nemo\n",
            "[NeMo I 2024-10-24 13:27:31 common:924] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2024-10-24 13:27:36 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpcvrenmrm/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:27:37 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
            "[NeMo W 2024-10-24 13:27:37 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    text_file: text_train.txt\n",
            "    labels_file: labels_train.txt\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "    \n",
            "[NeMo W 2024-10-24 13:27:37 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "    \n",
            "[NeMo W 2024-10-24 13:27:37 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "    \n",
            "[NeMo W 2024-10-24 13:27:37 nlp_overrides:802] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
            "    Megatron-based models require Apex to function correctly.\n",
            "[NeMo W 2024-10-24 13:27:37 lm_utils:91] bert-base-uncased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n",
            "[NeMo W 2024-10-24 13:27:39 modelPT:258] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:27:39 save_restore_connector:249] Model TokenClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/ner_en_bert/8186f86c83b11d70b43b9ead695e7eda/ner_en_bert.nemo.\n",
            "[NeMo I 2024-10-24 13:27:39 token_classification_dataset:123] Setting Max Seq length to: 16\n",
            "[NeMo I 2024-10-24 13:27:39 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
            "[NeMo I 2024-10-24 13:27:39 data_preprocessing:406] Min: 14 |                  Max: 16 |                  Mean: 15.0 |                  Median: 15.0\n",
            "[NeMo I 2024-10-24 13:27:39 data_preprocessing:412] 75 percentile: 15.50\n",
            "[NeMo I 2024-10-24 13:27:39 data_preprocessing:413] 99 percentile: 15.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:27:39 token_classification_dataset:152] 0 are longer than 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:27:39 token_classification_dataset:155] *** Example ***\n",
            "[NeMo I 2024-10-24 13:27:39 token_classification_dataset:156] i: 0\n",
            "[NeMo I 2024-10-24 13:27:39 token_classification_dataset:157] subtokens: [CLS] n ##vid ##ia released its new gp ##u in new york last september . [SEP]\n",
            "[NeMo I 2024-10-24 13:27:39 token_classification_dataset:158] loss_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "[NeMo I 2024-10-24 13:27:39 token_classification_dataset:159] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "[NeMo I 2024-10-24 13:27:39 token_classification_dataset:160] subtokens_mask: 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
            "\n",
            "Text: NVIDIA released its new GPU in New York last September.\n",
            "Entities: NVIDIA[B-ORG] released its new GPU in New[B-LOC] York[I-LOC] last September[B-TIME].\n",
            "\n",
            "Text: Jensen Huang discussed AI developments at GTC in San Francisco.\n",
            "Entities: Jensen[B-PER] Huang[I-PER] discussed AI developments at GTC[B-ORG] in San[B-LOC] Francisco[I-LOC].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QnA"
      ],
      "metadata": {
        "id": "lX0bEGnGDqRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.demonstrate_question_answering()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh3nfvGM4lVf",
        "outputId": "422901f2-b102-4dbb-a8dc-624c536dddbd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Question Answering Demo ===\n",
            "[NeMo I 2024-10-24 13:34:24 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.23.0/qa_squadv1.1_bertbase/ec1e93dd2fa3f3cbba91db856722c541/qa_squadv1.1_bertbase.nemo.\n",
            "[NeMo I 2024-10-24 13:34:24 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.23.0/qa_squadv1.1_bertbase/ec1e93dd2fa3f3cbba91db856722c541/qa_squadv1.1_bertbase.nemo\n",
            "[NeMo I 2024-10-24 13:34:24 common:924] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2024-10-24 13:34:30 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmp_rkzh00x/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:34:30 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
            "[NeMo W 2024-10-24 13:34:30 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    file: /datasets/squad/v1.1/train-v1.1.json\n",
            "    batch_size: 3\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    num_workers: 2\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2024-10-24 13:34:30 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    file: /datasets/squad/v1.1/dev-v1.1.json\n",
            "    batch_size: 3\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    num_workers: 2\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2024-10-24 13:34:30 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    file: null\n",
            "    batch_size: 24\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    num_workers: 2\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2024-10-24 13:34:30 nlp_overrides:802] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
            "    Megatron-based models require Apex to function correctly.\n",
            "[NeMo W 2024-10-24 13:34:30 lm_utils:91] bert-base-uncased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:34:32 bert_module:44] Restoring weights from /WS/bert/bert_base_uncased_mlm_final_1074591/BERT-STEP-2285714.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:34:32 bert_module:47] Path /WS/bert/bert_base_uncased_mlm_final_1074591/BERT-STEP-2285714.pt not found\n",
            "[NeMo W 2024-10-24 13:34:32 modelPT:258] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:34:33 save_restore_connector:249] Model QAModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/qa_squadv1.1_bertbase/ec1e93dd2fa3f3cbba91db856722c541/qa_squadv1.1_bertbase.nemo.\n",
            "\n",
            "Question: When was NVIDIA founded?\n",
            "Answer: 1993\n",
            "\n",
            "Question: What does NVIDIA design?\n",
            "Answer: graphics processing units\n",
            "\n",
            "Question: Where is NVIDIA headquartered?\n",
            "Answer: santa clara, california\n",
            "\n",
            "Question: Who is the founder of NVIDIA?\n",
            "Answer: 1993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Inspect model\n",
        "model = QAModel.from_pretrained(\"qa_squadv1.1_bertbase\")\n",
        "demo.inspect_qa_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWJgwofF9spD",
        "outputId": "68bb320c-2f12-4d9f-ecd9-c86585490f23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:36:55 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.23.0/qa_squadv1.1_bertbase/ec1e93dd2fa3f3cbba91db856722c541/qa_squadv1.1_bertbase.nemo.\n",
            "[NeMo I 2024-10-24 13:36:55 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.23.0/qa_squadv1.1_bertbase/ec1e93dd2fa3f3cbba91db856722c541/qa_squadv1.1_bertbase.nemo\n",
            "[NeMo I 2024-10-24 13:36:55 common:924] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2024-10-24 13:37:04 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpooytu3pc/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:37:04 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
            "[NeMo W 2024-10-24 13:37:04 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    file: /datasets/squad/v1.1/train-v1.1.json\n",
            "    batch_size: 3\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    num_workers: 2\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2024-10-24 13:37:04 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    file: /datasets/squad/v1.1/dev-v1.1.json\n",
            "    batch_size: 3\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    num_workers: 2\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2024-10-24 13:37:04 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    file: null\n",
            "    batch_size: 24\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    num_workers: 2\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2024-10-24 13:37:04 nlp_overrides:802] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
            "    Megatron-based models require Apex to function correctly.\n",
            "[NeMo W 2024-10-24 13:37:04 lm_utils:91] bert-base-uncased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:37:07 bert_module:44] Restoring weights from /WS/bert/bert_base_uncased_mlm_final_1074591/BERT-STEP-2285714.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:37:07 bert_module:47] Path /WS/bert/bert_base_uncased_mlm_final_1074591/BERT-STEP-2285714.pt not found\n",
            "[NeMo W 2024-10-24 13:37:07 modelPT:258] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:37:08 save_restore_connector:249] Model QAModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/qa_squadv1.1_bertbase/ec1e93dd2fa3f3cbba91db856722c541/qa_squadv1.1_bertbase.nemo.\n",
            "\n",
            "=== QA Model Inspection ===\n",
            "\n",
            "Public Methods:\n",
            "add_module(name: str, module: Optional[ForwardRef('Module')]) -> None\n",
            "all_gather(data: Union[torch.Tensor, Dict, List, Tuple], group: Optional[Any] = None, sync_grads: bool = False) -> Union[torch.Tensor, Dict, List, Tuple]\n",
            "apply(fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
            "backward(loss: torch.Tensor, *args: Any, **kwargs: Any) -> None\n",
            "bert_model(*args, **kwargs)\n",
            "bfloat16() -> ~T\n",
            "buffers(recurse: bool = True) -> Iterator[torch.Tensor]\n",
            "children() -> Iterator[ForwardRef('Module')]\n",
            "classifier(*args, **kwargs)\n",
            "clip_gradients(optimizer: torch.optim.optimizer.Optimizer, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None) -> None\n",
            "compile(*args, **kwargs)\n",
            "configure_callbacks() -> Union[Sequence[pytorch_lightning.callbacks.callback.Callback], pytorch_lightning.callbacks.callback.Callback]\n",
            "configure_gradient_clipping(optimizer: torch.optim.optimizer.Optimizer, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None) -> None\n",
            "configure_optimizers()\n",
            "configure_sharded_model() -> None\n",
            "cpu() -> typing_extensions.Self\n",
            "cuda(device=None)\n",
            "double() -> typing_extensions.Self\n",
            "eval() -> ~T\n",
            "export(output: str, input_example=None, verbose=False, do_constant_folding=True, onnx_opset_version=None, check_trace: Union[bool, List[torch.Tensor]] = False, dynamic_axes=None, check_tolerance=0.01, export_modules_as_functions=False, keep_initializers_as_inputs=None)\n",
            "extra_repr() -> str\n",
            "extract_state_dict_from(restore_path: 'str', save_dir: 'str', split_by_module: 'bool' = False, save_restore_connector: 'SaveRestoreConnector' = None)\n",
            "float() -> typing_extensions.Self\n",
            "forward(input_ids, attention_mask, token_type_ids)\n",
            "freeze() -> None\n",
            "from_config_dict(config: 'DictConfig', trainer: Optional[ForwardRef('Trainer')] = None)\n",
            "from_config_file(path2yaml_file: str)\n",
            "from_pretrained(model_name: str, refresh_cache: bool = False, override_config_path: Optional[str] = None, map_location: Optional[ForwardRef('torch.device')] = None, strict: bool = True, return_config: bool = False, trainer: Optional[ForwardRef('Trainer')] = None, save_restore_connector: nemo.core.connectors.save_restore_connector.SaveRestoreConnector = None)\n",
            "get_available_model_names() -> List[str]\n",
            "get_buffer(target: str) -> 'Tensor'\n",
            "get_export_config()\n",
            "get_export_subnet(subnet=None)\n",
            "get_extra_state() -> Any\n",
            "get_hf_model_filter() -> huggingface_hub.utils.endpoint_helpers.ModelFilter\n",
            "get_parameter(target: str) -> 'Parameter'\n",
            "get_submodule(target: str) -> 'Module'\n",
            "get_test_dataloader_prefix(dataloader_idx: 'int' = 0) -> 'str'\n",
            "get_validation_dataloader_prefix(dataloader_idx: 'int' = 0) -> 'str'\n",
            "half() -> typing_extensions.Self\n",
            "has_artifacts() -> 'bool'\n",
            "has_native_or_submodules_artifacts() -> 'bool'\n",
            "has_nemo_submodules() -> 'bool'\n",
            "inference(file: str, batch_size: int = 1, num_samples: int = -1, output_nbest_file: Optional[str] = None, output_prediction_file: Optional[str] = None)\n",
            "input_module(*args, **kwargs)\n",
            "ipu(device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            "is_model_parallel_initialized: Could not get signature - 'QAModel' object has no attribute 'is_model_parallel_initialized'\n",
            "list_available_models() -> Optional[nemo.core.classes.common.PretrainedModelInfo]\n",
            "list_export_subnets()\n",
            "load_from_checkpoint(checkpoint_path: str, map_location: Any = None, hparams_file: Optional[str] = None, strict: bool = True, **kwargs)\n",
            "load_part_of_state_dict(state_dict, include, exclude, load_from_string=None)\n",
            "load_state_dict(state_dict: Mapping[str, Any], strict: bool = True)\n",
            "log(name: str, value: Union[torchmetrics.metric.Metric, torch.Tensor, int, float], prog_bar: bool = False, logger: Optional[bool] = None, on_step: Optional[bool] = None, on_epoch: Optional[bool] = None, reduce_fx: Union[str, Callable] = 'mean', enable_graph: bool = False, sync_dist: bool = False, sync_dist_group: Optional[Any] = None, add_dataloader_idx: bool = True, batch_size: Optional[int] = None, metric_attribute: Optional[str] = None, rank_zero_only: bool = False) -> None\n",
            "log_dict(dictionary: Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, int, float]], prog_bar: bool = False, logger: Optional[bool] = None, on_step: Optional[bool] = None, on_epoch: Optional[bool] = None, reduce_fx: Union[str, Callable] = 'mean', enable_graph: bool = False, sync_dist: bool = False, sync_dist_group: Optional[Any] = None, add_dataloader_idx: bool = True, batch_size: Optional[int] = None, rank_zero_only: bool = False) -> None\n",
            "loss(*args, **kwargs)\n",
            "lr_scheduler_step(scheduler: Union[torch.optim.lr_scheduler.LRScheduler, torch.optim.lr_scheduler.ReduceLROnPlateau], metric: Optional[Any]) -> None\n",
            "lr_schedulers() -> Union[NoneType, List[Union[lightning_fabric.utilities.types.LRScheduler, lightning_fabric.utilities.types.ReduceLROnPlateau]], lightning_fabric.utilities.types.LRScheduler, lightning_fabric.utilities.types.ReduceLROnPlateau]\n",
            "manual_backward(loss: torch.Tensor, *args: Any, **kwargs: Any) -> None\n",
            "maybe_init_from_pretrained_checkpoint(cfg: 'OmegaConf', map_location: 'str' = 'cpu')\n",
            "modules() -> Iterator[ForwardRef('Module')]\n",
            "mtia(device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            "multi_test_epoch_end(outputs: 'List[Dict[str, torch.Tensor]]', dataloader_idx: 'int' = 0) -> 'Optional[Dict[str, Dict[str, torch.Tensor]]]'\n",
            "multi_validation_epoch_end(outputs: 'List[Dict[str, torch.Tensor]]', dataloader_idx: 'int' = 0) -> 'Optional[Dict[str, Dict[str, torch.Tensor]]]'\n",
            "named_buffers(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
            "named_children() -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
            "named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
            "named_nemo_modules(prefix_name: 'str' = '', prefix_config: 'str' = '') -> \"Iterator[Tuple[str, str, 'ModelPT']]\"\n",
            "named_parameters(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
            "on_after_backward() -> None\n",
            "on_after_batch_transfer(batch: Any, dataloader_idx: int) -> Any\n",
            "on_before_backward(loss: torch.Tensor) -> None\n",
            "on_before_batch_transfer(batch: Any, dataloader_idx: int) -> Any\n",
            "on_before_optimizer_step(optimizer: torch.optim.optimizer.Optimizer) -> None\n",
            "on_before_zero_grad(optimizer: torch.optim.optimizer.Optimizer) -> None\n",
            "on_fit_end() -> None\n",
            "on_fit_start() -> 'None'\n",
            "on_load_checkpoint(checkpoint: Dict[str, Any]) -> None\n",
            "on_predict_batch_end(outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
            "on_predict_batch_start(batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
            "on_predict_end()\n",
            "on_predict_epoch_end() -> None\n",
            "on_predict_epoch_start() -> None\n",
            "on_predict_model_eval() -> None\n",
            "on_predict_start() -> None\n",
            "on_save_checkpoint(checkpoint: Dict[str, Any]) -> None\n",
            "on_test_batch_end(outputs: Union[torch.Tensor, Dict[str, Any], NoneType], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
            "on_test_batch_start(batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
            "on_test_end()\n",
            "on_test_epoch_end()\n",
            "on_test_epoch_start() -> None\n",
            "on_test_model_eval() -> None\n",
            "on_test_model_train() -> None\n",
            "on_test_start() -> None\n",
            "on_train_batch_end(outputs, batch: 'Any', batch_idx: 'int', unused: 'int' = 0) -> 'None'\n",
            "on_train_batch_start(batch: 'Any', batch_idx: 'int', unused: 'int' = 0) -> 'Optional[int]'\n",
            "on_train_end()\n",
            "on_train_epoch_end() -> None\n",
            "on_train_epoch_start() -> None\n",
            "on_train_start()\n",
            "on_validation_batch_end(outputs: Union[torch.Tensor, Dict[str, Any], NoneType], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
            "on_validation_batch_start(batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
            "on_validation_end() -> None\n",
            "on_validation_epoch_end()\n",
            "on_validation_epoch_start() -> None\n",
            "on_validation_model_eval() -> None\n",
            "on_validation_model_train() -> None\n",
            "on_validation_start() -> None\n",
            "optimizer_step(epoch: int, batch_idx: int, optimizer: Union[torch.optim.optimizer.Optimizer, pytorch_lightning.core.optimizer.LightningOptimizer], optimizer_closure: Optional[Callable[[], Any]] = None) -> None\n",
            "optimizer_zero_grad(epoch: int, batch_idx: int, optimizer: torch.optim.optimizer.Optimizer) -> None\n",
            "optimizers(use_pl_optimizer: bool = True) -> Union[torch.optim.optimizer.Optimizer, pytorch_lightning.core.optimizer.LightningOptimizer, lightning_fabric.wrappers._FabricOptimizer, List[torch.optim.optimizer.Optimizer], List[pytorch_lightning.core.optimizer.LightningOptimizer], List[lightning_fabric.wrappers._FabricOptimizer]]\n",
            "output_module(*args, **kwargs)\n",
            "parameters(recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
            "predict_dataloader() -> Any\n",
            "predict_step(batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any\n",
            "prepare_data() -> None\n",
            "prepare_test(trainer: \"'Trainer'\") -> 'bool'\n",
            "print(*args: Any, **kwargs: Any) -> None\n",
            "register_artifact(config_path: str, src: str, verify_src_exists: bool = False)\n",
            "register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
            "register_bert_model()\n",
            "register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
            "register_forward_hook(hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            "register_forward_pre_hook(hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            "register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            "register_full_backward_pre_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            "register_load_state_dict_post_hook(hook)\n",
            "register_load_state_dict_pre_hook(hook)\n",
            "register_module(name: str, module: Optional[ForwardRef('Module')]) -> None\n",
            "register_nemo_submodule(name: 'str', config_field: 'str', model: \"'ModelPT'\") -> 'None'\n",
            "register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
            "register_state_dict_post_hook(hook)\n",
            "register_state_dict_pre_hook(hook)\n",
            "requires_grad_(requires_grad: bool = True) -> ~T\n",
            "restore_from(restore_path: str, override_config_path: Union[omegaconf.omegaconf.OmegaConf, str, NoneType] = None, map_location: Optional[torch.device] = None, strict: bool = True, return_config: bool = False, save_restore_connector: nemo.core.connectors.save_restore_connector.SaveRestoreConnector = None, trainer: Optional[pytorch_lightning.trainer.trainer.Trainer] = None)\n",
            "save_hyperparameters(*args: Any, ignore: Union[Sequence[str], str, NoneType] = None, frame: Optional[frame] = None, logger: bool = True) -> None\n",
            "save_to(save_path: 'str')\n",
            "search_huggingface_models(model_filter: Union[huggingface_hub.utils.endpoint_helpers.ModelFilter, List[huggingface_hub.utils.endpoint_helpers.ModelFilter], NoneType] = None) -> List[huggingface_hub.hf_api.ModelInfo]\n",
            "set_export_config(args)\n",
            "set_extra_state(state: Any) -> None\n",
            "set_submodule(target: str, module: 'Module') -> None\n",
            "set_trainer(trainer: 'Trainer')\n",
            "set_world_size(trainer: 'Trainer')\n",
            "setup(stage: 'Optional[str]' = None)\n",
            "setup_multiple_test_data(test_data_config: 'Union[DictConfig, Dict]')\n",
            "setup_multiple_validation_data(val_data_config: 'Union[DictConfig, Dict]')\n",
            "setup_optimization(optim_config: 'Optional[Union[DictConfig, Dict]]' = None, optim_kwargs: 'Optional[Dict[str, Any]]' = None)\n",
            "setup_optimizer_param_groups()\n",
            "setup_test_data(test_data_config: Optional[omegaconf.dictconfig.DictConfig])\n",
            "setup_tokenizer(cfg: omegaconf.dictconfig.DictConfig)\n",
            "setup_training_data(train_data_config: Optional[omegaconf.dictconfig.DictConfig])\n",
            "setup_validation_data(val_data_config: Optional[omegaconf.dictconfig.DictConfig])\n",
            "share_memory() -> ~T\n",
            "state_dict(*args, destination=None, prefix='', keep_vars=False)\n",
            "summarize(max_depth: 'int' = 1) -> 'model_summary.ModelSummary'\n",
            "teardown(stage: 'str')\n",
            "test_dataloader()\n",
            "test_step(batch, batch_idx)\n",
            "to(*args: Any, **kwargs: Any) -> typing_extensions.Self\n",
            "to_config_dict() -> 'DictConfig'\n",
            "to_config_file(path2yaml_file: str)\n",
            "to_empty(*, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
            "to_onnx(file_path: Union[str, pathlib.Path], input_sample: Optional[Any] = None, **kwargs: Any) -> None\n",
            "to_torchscript(file_path: Union[str, pathlib.Path, NoneType] = None, method: Optional[str] = 'script', example_inputs: Optional[Any] = None, **kwargs: Any) -> Union[torch.ScriptModule, Dict[str, torch.ScriptModule]]\n",
            "toggle_optimizer(optimizer: Union[torch.optim.optimizer.Optimizer, pytorch_lightning.core.optimizer.LightningOptimizer]) -> None\n",
            "train(mode: bool = True) -> ~T\n",
            "train_dataloader()\n",
            "training_step(batch, batch_idx)\n",
            "training_step(batch, batch_idx)\n",
            "transfer_batch_to_device(batch: Any, device: torch.device, dataloader_idx: int) -> Any\n",
            "type(dst_type: Union[str, torch.dtype]) -> typing_extensions.Self\n",
            "unfreeze() -> None\n",
            "untoggle_optimizer(optimizer: Union[torch.optim.optimizer.Optimizer, pytorch_lightning.core.optimizer.LightningOptimizer]) -> None\n",
            "update_save_restore_connector(save_restore_connector)\n",
            "val_dataloader()\n",
            "validation_step(batch, batch_idx)\n",
            "xpu(device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            "zero_grad(set_to_none: bool = True) -> None\n",
            "\n",
            "Model Configuration:\n",
            "{'nemo_path': '/results//model.nemo', 'dataset': {'version_2_with_negative': False, 'doc_stride': 128, 'max_query_length': 64, 'max_seq_length': 512, 'max_answer_length': 30, 'null_score_diff_threshold': 0.0, 'n_best_size': 20, 'use_cache': False, 'do_lower_case': True, 'num_workers': 2, 'pin_memory': False, 'drop_last': False}, 'train_ds': {'file': '/datasets/squad/v1.1/train-v1.1.json', 'batch_size': 3, 'shuffle': True, 'num_samples': -1, 'num_workers': 2, 'drop_last': False, 'pin_memory': False}, 'validation_ds': {'file': '/datasets/squad/v1.1/dev-v1.1.json', 'batch_size': 3, 'shuffle': False, 'num_samples': -1, 'num_workers': 2, 'drop_last': False, 'pin_memory': False}, 'test_ds': {'file': None, 'batch_size': 24, 'shuffle': False, 'num_samples': -1, 'num_workers': 2, 'drop_last': False, 'pin_memory': False}, 'tokenizer': {'tokenizer_name': 'bert-base-uncased', 'vocab_file': 'tokenizer.vocab_file', 'tokenizer_model': None, 'special_tokens': None}, 'language_model': {'pretrained_model_name': 'bert-base-uncased', 'lm_checkpoint': '/WS/bert/bert_base_uncased_mlm_final_1074591/BERT-STEP-2285714.pt', 'config_file': '/root/.cache/huggingface/nemo_nlp_tmp/_encoder_config.json', 'config': None}, 'token_classifier': {'num_layers': 1, 'dropout': 0.0, 'num_classes': 2, 'activation': 'relu', 'log_softmax': False, 'use_transformer_init': True}, 'optim': {'name': 'adamw', 'lr': 3e-05, 'weight_decay': 0.0, 'sched': {'name': 'SquareRootAnnealing', 'monitor': 'val_loss', 'reduce_on_plateau': False, 'warmup_steps': None, 'warmup_ratio': 0.0, 'last_epoch': -1}}, 'target': 'nemo.collections.nlp.models.question_answering.qa_model.QAModel', 'nemo_version': '1.23.0'}\n",
            "\n",
            "Model Parameters:\n",
            "bert_model.embeddings.word_embeddings.weight: torch.Size([30522, 768])\n",
            "bert_model.embeddings.position_embeddings.weight: torch.Size([512, 768])\n",
            "bert_model.embeddings.token_type_embeddings.weight: torch.Size([2, 768])\n",
            "bert_model.embeddings.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.embeddings.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
            "bert_model.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
            "bert_model.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
            "bert_model.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
            "bert_model.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
            "bert_model.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
            "bert_model.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
            "bert_model.pooler.dense.weight: torch.Size([768, 768])\n",
            "bert_model.pooler.dense.bias: torch.Size([768])\n",
            "classifier.mlp.layer0.weight: torch.Size([2, 768])\n",
            "classifier.mlp.layer0.bias: torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification"
      ],
      "metadata": {
        "id": "mk2W13W5KFFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.demonstrate_text_classification()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_vOSdU84ol6",
        "outputId": "973411cb-2f03-4c0e-ce4d-75d781353945"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Text Classification Demo ===\n",
            "No Text Classification models available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punctuation and Capitalization"
      ],
      "metadata": {
        "id": "bbi8AqpIKOiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.demonstrate_punctuation_capitalization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huQnp-tHKEip",
        "outputId": "a4b9825a-39bc-45fa-fcfd-94ed724623f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Punctuation and Capitalization Demo ===\n",
            "[NeMo I 2024-10-24 13:38:57 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/punctuation_en_bert/versions/1.0.0rc1/files/punctuation_en_bert.nemo to /root/.cache/torch/NeMo/NeMo_1.23.0/punctuation_en_bert/93b0369b5e0d147f61895feffcbcfb88/punctuation_en_bert.nemo\n",
            "[NeMo I 2024-10-24 13:39:04 common:924] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2024-10-24 13:39:09 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmppdf96h1h/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:39:09 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
            "[NeMo W 2024-10-24 13:39:09 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    use_audio: false\n",
            "    audio_file: null\n",
            "    sample_rate: 16000\n",
            "    use_bucketing: true\n",
            "    batch_size: 32\n",
            "    preload_audios: true\n",
            "    use_tarred_dataset: false\n",
            "    label_info_save_dir: null\n",
            "    text_file: text_train.txt\n",
            "    labels_file: labels_train.txt\n",
            "    tokens_in_batch: null\n",
            "    max_seq_length: 128\n",
            "    num_samples: -1\n",
            "    use_cache: true\n",
            "    cache_dir: null\n",
            "    get_label_frequences: false\n",
            "    verbose: true\n",
            "    n_jobs: 0\n",
            "    tar_metadata_file: null\n",
            "    tar_shuffle_n: 1\n",
            "    shard_strategy: scatter\n",
            "    shuffle: true\n",
            "    drop_last: false\n",
            "    pin_memory: true\n",
            "    num_workers: 8\n",
            "    persistent_workers: true\n",
            "    ds_item: punct_dataset_complete\n",
            "    \n",
            "[NeMo W 2024-10-24 13:39:09 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    use_audio: false\n",
            "    audio_file: null\n",
            "    sample_rate: 16000\n",
            "    use_bucketing: true\n",
            "    batch_size: 32\n",
            "    preload_audios: true\n",
            "    use_tarred_dataset: false\n",
            "    label_info_save_dir: null\n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    tokens_in_batch: null\n",
            "    max_seq_length: 128\n",
            "    num_samples: -1\n",
            "    use_cache: true\n",
            "    cache_dir: null\n",
            "    get_label_frequences: false\n",
            "    verbose: true\n",
            "    n_jobs: 0\n",
            "    tar_metadata_file: null\n",
            "    tar_shuffle_n: 1\n",
            "    shard_strategy: scatter\n",
            "    shuffle: true\n",
            "    drop_last: false\n",
            "    pin_memory: true\n",
            "    num_workers: 8\n",
            "    persistent_workers: true\n",
            "    ds_item: punct_dataset_complete\n",
            "    \n",
            "[NeMo W 2024-10-24 13:39:09 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    use_audio: false\n",
            "    audio_file: null\n",
            "    sample_rate: 16000\n",
            "    use_bucketing: true\n",
            "    batch_size: 32\n",
            "    preload_audios: true\n",
            "    use_tarred_dataset: false\n",
            "    label_info_save_dir: null\n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    tokens_in_batch: null\n",
            "    max_seq_length: 128\n",
            "    num_samples: -1\n",
            "    use_cache: true\n",
            "    cache_dir: null\n",
            "    get_label_frequences: false\n",
            "    verbose: true\n",
            "    n_jobs: 0\n",
            "    tar_metadata_file: null\n",
            "    tar_shuffle_n: 1\n",
            "    shard_strategy: scatter\n",
            "    shuffle: true\n",
            "    drop_last: false\n",
            "    pin_memory: true\n",
            "    num_workers: 8\n",
            "    persistent_workers: true\n",
            "    ds_item: punct_dataset_complete\n",
            "    \n",
            "[NeMo W 2024-10-24 13:39:09 nlp_overrides:802] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
            "    Megatron-based models require Apex to function correctly.\n",
            "[NeMo W 2024-10-24 13:39:09 save_restore_connector:394] src path does not exist or it is not a path in nemo file. src value I got was: bert-base-uncased_encoder_config.json. Absolute: /content/bert-base-uncased_encoder_config.json\n",
            "[NeMo W 2024-10-24 13:39:09 lm_utils:91] bert-base-uncased is not in get_pretrained_lm_models_list(include_external=False), will be using AutoModel from HuggingFace.\n",
            "[NeMo W 2024-10-24 13:39:10 save_restore_connector:394] src path does not exist or it is not a path in nemo file. src value I got was: punct_label_ids.csv. Absolute: /content/punct_label_ids.csv\n",
            "[NeMo W 2024-10-24 13:39:10 punctuation_capitalization_model:719] The artifact `class_labels.punct_labels_file` was not found in checkpoint. Will rely on `punct_label_ids` parameter\n",
            "[NeMo W 2024-10-24 13:39:10 save_restore_connector:394] src path does not exist or it is not a path in nemo file. src value I got was: capit_label_ids.csv. Absolute: /content/capit_label_ids.csv\n",
            "[NeMo W 2024-10-24 13:39:10 punctuation_capitalization_model:741] The artifact `class_labels.capit_labels_file` was not found in checkpoint. Will rely on `capit_label_ids` parameter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:39:10 save_restore_connector:249] Model PunctuationCapitalizationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/punctuation_en_bert/93b0369b5e0d147f61895feffcbcfb88/punctuation_en_bert.nemo.\n",
            "\n",
            "Processing texts...\n",
            "[NeMo I 2024-10-24 13:39:10 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
            "[NeMo I 2024-10-24 13:39:10 punctuation_capitalization_infer_dataset:127] Max length: 18\n",
            "[NeMo I 2024-10-24 13:39:10 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
            "[NeMo I 2024-10-24 13:39:10 data_preprocessing:406] Min: 16 |                  Max: 16 |                  Mean: 16.0 |                  Median: 16.0\n",
            "[NeMo I 2024-10-24 13:39:10 data_preprocessing:412] 75 percentile: 16.00\n",
            "[NeMo I 2024-10-24 13:39:10 data_preprocessing:413] 99 percentile: 16.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 18.35batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original: nvidia is a technology company based in santa clara california that develops gpus\n",
            "Processed: Nvidia is a technology company based in Santa Clara, California, that develops Gpus.\n",
            "[NeMo I 2024-10-24 13:39:10 punctuation_capitalization_model:1167] Using batch size 1 for inference\n",
            "[NeMo I 2024-10-24 13:39:10 punctuation_capitalization_infer_dataset:127] Max length: 11\n",
            "[NeMo I 2024-10-24 13:39:10 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
            "[NeMo I 2024-10-24 13:39:10 data_preprocessing:406] Min: 9 |                  Max: 9 |                  Mean: 9.0 |                  Median: 9.0\n",
            "[NeMo I 2024-10-24 13:39:10 data_preprocessing:412] 75 percentile: 9.00\n",
            "[NeMo I 2024-10-24 13:39:10 data_preprocessing:413] 99 percentile: 9.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 71.89batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original: ai and deep learning are transforming various industries globally\n",
            "Processed: Ai and deep Learning are transforming various industries globally.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation"
      ],
      "metadata": {
        "id": "U84_CGCSLOH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.demonstrate_machine_translation()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXWigcmKKNtX",
        "outputId": "0e802ca0-eed7-4b38-9ab5-0f05629183c5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Machine Translation Demo ===\n",
            "[NeMo I 2024-10-24 13:43:06 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/nmt_en_zh_transformer24x6/versions/1.5/files/en_zh_24x6.nemo to /root/.cache/torch/NeMo/NeMo_1.23.0/en_zh_24x6/e514abee8ddb6edba76343b6a5b53394/en_zh_24x6.nemo\n",
            "[NeMo I 2024-10-24 13:43:29 common:924] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2024-10-24 13:43:55 tokenizer_utils:179] Getting YouTokenToMeTokenizer with model: /tmp/tmp88hs7z3r/75ff3fef67d84ce59bb34e2c0299c9b2_tokenizer.encoder.32000.BPE.model with r2l: False.\n",
            "[NeMo I 2024-10-24 13:43:55 tokenizer_utils:179] Getting YouTokenToMeTokenizer with model: /tmp/tmp88hs7z3r/d65d217cc9984019b65cf43cf579561a_tokenizer.decoder.32000.BPE.model with r2l: False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:43:55 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    src_file_name: null\n",
            "    tgt_file_name: null\n",
            "    use_tarred_dataset: true\n",
            "    tar_files:\n",
            "    - /data/tarred_dataset_old_plus_paracrawl_4k_tokens/parallel.batches.tokens.4000._OP_0..769_CL_.tar\n",
            "    - /data/tarred_dataset_old_plus_paracrawl_en_zh_r2l_distilled_4k_tokens/parallel.batches.tokens.4000._OP_0..745_CL_.tar\n",
            "    metadata_file:\n",
            "    - /data/tarred_dataset_old_plus_paracrawl_4k_tokens/metadata.tokens.4000.json\n",
            "    - /data/tarred_dataset_old_plus_paracrawl_en_zh_r2l_distilled_4k_tokens/metadata.tokens.4000.json\n",
            "    lines_per_dataset_fragment: 1000000\n",
            "    num_batches_per_tarfile: 100\n",
            "    shard_strategy: scatter\n",
            "    tokens_in_batch: 512\n",
            "    clean: true\n",
            "    max_seq_length: 512\n",
            "    min_seq_length: 1\n",
            "    cache_ids: false\n",
            "    cache_data_per_node: false\n",
            "    use_cache: false\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    num_workers: 8\n",
            "    reverse_lang_direction: false\n",
            "    load_from_tarred_dataset: false\n",
            "    metadata_path: null\n",
            "    tar_shuffle_n: 100\n",
            "    n_preproc_jobs: -2\n",
            "    tar_file_prefix: parallel\n",
            "    concat_sampling_technique: random\n",
            "    concat_sampling_temperature: 5\n",
            "    concat_sampling_probabilities:\n",
            "    - 0.6\n",
            "    - 0.4\n",
            "    \n",
            "[NeMo W 2024-10-24 13:43:55 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    src_file_name:\n",
            "    - /data/wmt19-en-zh.clean.tok.src\n",
            "    - /data/wmt20-en-zh.clean.tok.src\n",
            "    - /data/wmt18-en-zh.clean.tok.src\n",
            "    tgt_file_name:\n",
            "    - /data/wmt19-en-zh.clean.tok.ref\n",
            "    - /data/wmt20-en-zh.clean.tok.ref\n",
            "    - /data/wmt18-en-zh.clean.tok.ref\n",
            "    use_tarred_dataset: false\n",
            "    tar_files: null\n",
            "    metadata_file: null\n",
            "    lines_per_dataset_fragment: 1000000\n",
            "    num_batches_per_tarfile: 1000\n",
            "    shard_strategy: scatter\n",
            "    tokens_in_batch: 512\n",
            "    clean: false\n",
            "    max_seq_length: 512\n",
            "    min_seq_length: 1\n",
            "    cache_ids: false\n",
            "    cache_data_per_node: false\n",
            "    use_cache: false\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    num_workers: 8\n",
            "    reverse_lang_direction: false\n",
            "    load_from_tarred_dataset: false\n",
            "    metadata_path: null\n",
            "    tar_shuffle_n: 100\n",
            "    n_preproc_jobs: -2\n",
            "    tar_file_prefix: parallel\n",
            "    concat_sampling_technique: temperature\n",
            "    concat_sampling_temperature: 5\n",
            "    concat_sampling_probabilities: null\n",
            "    \n",
            "[NeMo W 2024-10-24 13:43:55 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    src_file_name: /data/wmt19-en-zh.clean.tok.src\n",
            "    tgt_file_name: /data/wmt19-en-zh.clean.tok.src\n",
            "    use_tarred_dataset: false\n",
            "    tar_files: null\n",
            "    metadata_file: null\n",
            "    lines_per_dataset_fragment: 1000000\n",
            "    num_batches_per_tarfile: 1000\n",
            "    shard_strategy: scatter\n",
            "    tokens_in_batch: 512\n",
            "    clean: false\n",
            "    max_seq_length: 512\n",
            "    min_seq_length: 1\n",
            "    cache_ids: false\n",
            "    cache_data_per_node: false\n",
            "    use_cache: false\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    num_workers: 8\n",
            "    reverse_lang_direction: false\n",
            "    load_from_tarred_dataset: false\n",
            "    metadata_path: null\n",
            "    tar_shuffle_n: 100\n",
            "    n_preproc_jobs: -2\n",
            "    tar_file_prefix: parallel\n",
            "    concat_sampling_technique: temperature\n",
            "    concat_sampling_temperature: 5\n",
            "    concat_sampling_probabilities: null\n",
            "    \n",
            "[NeMo W 2024-10-24 13:43:55 nlp_overrides:802] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
            "    Megatron-based models require Apex to function correctly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:44:04 save_restore_connector:249] Model MTEncDecModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/en_zh_24x6/e514abee8ddb6edba76343b6a5b53394/en_zh_24x6.nemo.\n",
            "\n",
            "Translating texts...\n",
            "\n",
            "English: NVIDIA GPUs are excellent for deep learning.\n",
            "Translated: NVIDIA GPU 非常适合深度学习。\n",
            "\n",
            "English: Artificial intelligence is transforming the technology landscape.\n",
            "Translated: 人工智能正在改变技术格局。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to Speech"
      ],
      "metadata": {
        "id": "UoyYELWdMBIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.demonstrate_text_to_speech()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "65ynRPoRKaxB",
        "outputId": "accc12dc-fac7-4dcc-bc4a-d112bdeb9068"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Text-to-Speech Demo ===\n",
            "[NeMo I 2024-10-24 13:49:09 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.\n",
            "[NeMo I 2024-10-24 13:49:09 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo\n",
            "[NeMo I 2024-10-24 13:49:09 common:924] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
            "INFO:NeMo-text-processing:Creating ClassifyFst grammars.\n",
            "[NeMo W 2024-10-24 13:49:40 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.\n",
            "[NeMo W 2024-10-24 13:49:40 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
            "      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_train_clean_ngc.json\n",
            "      sample_rate: 22050\n",
            "      sup_data_path: /raid/LJSpeech/supplementary\n",
            "      sup_data_types:\n",
            "      - align_prior_matrix\n",
            "      - pitch\n",
            "      n_fft: 1024\n",
            "      win_length: 1024\n",
            "      hop_length: 256\n",
            "      window: hann\n",
            "      n_mels: 80\n",
            "      lowfreq: 0\n",
            "      highfreq: 8000\n",
            "      max_duration: null\n",
            "      min_duration: 0.1\n",
            "      ignore_file: null\n",
            "      trim: false\n",
            "      pitch_fmin: 65.40639132514966\n",
            "      pitch_fmax: 2093.004522404789\n",
            "      pitch_norm: true\n",
            "      pitch_mean: 212.35873413085938\n",
            "      pitch_std: 68.52806091308594\n",
            "      use_beta_binomial_interpolator: true\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: true\n",
            "      batch_size: 24\n",
            "      num_workers: 0\n",
            "    \n",
            "[NeMo W 2024-10-24 13:49:40 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
            "      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_val_clean_ngc.json\n",
            "      sample_rate: 22050\n",
            "      sup_data_path: /raid/LJSpeech/supplementary\n",
            "      sup_data_types:\n",
            "      - align_prior_matrix\n",
            "      - pitch\n",
            "      n_fft: 1024\n",
            "      win_length: 1024\n",
            "      hop_length: 256\n",
            "      window: hann\n",
            "      n_mels: 80\n",
            "      lowfreq: 0\n",
            "      highfreq: 8000\n",
            "      max_duration: null\n",
            "      min_duration: null\n",
            "      ignore_file: null\n",
            "      trim: false\n",
            "      pitch_fmin: 65.40639132514966\n",
            "      pitch_fmax: 2093.004522404789\n",
            "      pitch_norm: true\n",
            "      pitch_mean: 212.35873413085938\n",
            "      pitch_std: 68.52806091308594\n",
            "      use_beta_binomial_interpolator: true\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: false\n",
            "      batch_size: 24\n",
            "      num_workers: 0\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:49:40 features:289] PADDING: 1\n",
            "[NeMo I 2024-10-24 13:49:40 save_restore_connector:249] Model FastPitchModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/tts_en_fastpitch_align/b7d086a07b5126c12d5077d9a641a38c/tts_en_fastpitch_align.nemo.\n",
            "[NeMo I 2024-10-24 13:49:40 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_1.23.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
            "[NeMo I 2024-10-24 13:49:40 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.23.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo\n",
            "[NeMo I 2024-10-24 13:49:40 common:924] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:49:47 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
            "      manifest_filepath: /home/fkreuk/data/train_finetune.txt\n",
            "      min_duration: 0.75\n",
            "      n_segments: 8192\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: true\n",
            "      batch_size: 64\n",
            "      num_workers: 4\n",
            "    \n",
            "[NeMo W 2024-10-24 13:49:47 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    dataset:\n",
            "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
            "      manifest_filepath: /home/fkreuk/data/val_finetune.txt\n",
            "      min_duration: 3\n",
            "      n_segments: 66150\n",
            "    dataloader_params:\n",
            "      drop_last: false\n",
            "      shuffle: false\n",
            "      batch_size: 5\n",
            "      num_workers: 4\n",
            "    \n",
            "[NeMo W 2024-10-24 13:49:47 features:266] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:49:47 features:289] PADDING: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-24 13:49:47 features:266] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-24 13:49:47 features:289] PADDING: 0\n",
            "[NeMo I 2024-10-24 13:49:48 save_restore_connector:249] Model HifiGanModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo.\n",
            "Converting text to speech: 'How are you, guys?'\n",
            "✓ Audio generation successful!\n",
            "Note: Audio playback is not available in this environment\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cleanup\n",
        "print(\"\\n=== Cleanup ===\")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGUpTrVS9mbh",
        "outputId": "84b7df8b-efc1-4854-87af-230f26fe1867"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Cleanup ===\n"
          ]
        }
      ]
    }
  ]
}